\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\title{MAC0460 - Introdução ao Aprendizado de Máquina \\ EP03}
\author{Matheus Santos - 10297672 \\
        Raul Reis - 8535596}

\begin{document}
\makeatletter
\begin{center}
	{\Large\textbf \@title \\ }
	{\Large \@author \\ }
\end{center}

\vspace{1.1cm}

\section{Regressão logística}

Regressão Ligistica:

A saída do modelo de regressão logística é uma probabilidade, o que é
ideal para modelos que exigem mais do que uma decisão binária.

O modelo de classificação linear usa um limite bem estabelicido no sinal
da saída. Para um dado vetor usando a função sign temos como resultado 1 ou -1.

A regressão linear não tem nenhuma restrição.

O modelo de regressão logística esta entre esses dois modelos pois queremos restringir
a saída em valores entre 0 e 1. Usamos a função logistica para isso.

Esse resultado pode ser interpretado como uma probabilidade para uma decisão binária.


Gradiente descendente:


A ideia é minimizar uma função para achar um mínimo local (talvez global), o valor
mínimo achado depende dos pesos inciais. Mas como usaremos a regressão logística com
a função de erro cross-entropy teremos garantido um mínimo global independente dos 
valores do peso inicial, já que a função cross-entropy é convexa.

Função croos-entropy está definida no livro na página 95 assim como o algoritmo do
gradiente descendente.

Testes: 

O algoritmo de teste, encontrado no arquivo util.py, gera dois aglomerados de distribuição normal multivariada (com centro diferentes). É possível perceber que os erros de classificação do algoritmo aumenta conforme modificamos determinados parâmetros como o aumento acessivo do numero de iterações, ou diminuindo o valor de batch\_size, fazendo com que o resultado, em duas dimensões, formasse uma reta decrescente. Ao diminuir ou aumentar o learning\_rate, temos que o erro de classificação aumenta, portanto precisamos incluir um número intermediário para ter um resultado plausível (pelo anunciado foi escolhido o 1e-2). 

\end{document}

